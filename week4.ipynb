{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Regression Mining\n",
    "\n",
    "### What's on this week\n",
    "1. [Resuming from week 3](#resume)\n",
    "2. [Building your first logistic regression model](#build)\n",
    "3. [Understanding your logistic regression model](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "5. [Feature selection](#fselect)\n",
    "\n",
    "---\n",
    "\n",
    "The practical note for this week introduces you to regression mining in Python, particularly using logistic regression. Regressions are a class of linear models that learn coefficients associated with each variable/field and uses them to make predictions.\n",
    "\n",
    "**This tutorial notes is in experimental version. Please give us feedbacks and suggestions on how to make it better. Ask your tutor for any question and clarification.**\n",
    "\n",
    "## 1. Resuming from week 3 <a name=\"resume\"></a>\n",
    "Last week, we learned how to perform data mining with decision trees in Python. For this week, we will reuse the code for data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dm_tools import data_prep\n",
    "\n",
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# train test split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building your logistic regression <a name=\"build\"></a>\n",
    "\n",
    "### 2.1. Scaling your input\n",
    "\n",
    "Regression models are sensitive to extreme or outlying values in the input space. Inputs with highly skewed or kurtotic distributions are often selected over inputs with better overall predictions. To avoid this problem, we can scale our inputs first before building our logistic regression model. In `sklearn`, this can easily be done using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Building logistic regression\n",
    "Once we scaled our inputs, we are ready to build the model. There are a number of types of regression, namely linear and logistic. The type of regression to use is determined by the target's measurement level. In this case study, the target is of categorical type, thus we need to use logistic regression.\n",
    "\n",
    "Import and train your logistic regression using code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.601486681809\n",
      "0.560396448482\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.57      0.56      2422\n",
      "          1       0.56      0.56      0.56      2421\n",
      "\n",
      "avg / total       0.56      0.56      0.56      4843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score of this model shows an improvement over our tuned decision tree model from last week. We will tune this logistic regression model later using GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameters with GridSearchCV\n",
    "\n",
    "Alright, let's see whether we can tune our logistic regression model to be better. In this example, I will tune it using only one parameter, `C`, which is the inverse of regularization strength. Smaller values specify stronger regularization. Typical values for C range from 10^-6 to 10^4, increasing in order or 10, which is what we will use here.\n",
    "\n",
    "Tips: sometimes `GridSearchCV` can be very slow if we are searching over a large set of possible values. To aid with this problem, `GridSearchCV` is implemented with parallel running capability and you can specify how many parallel processes running in the same time with `n_jobs` (-1 means GridSearchCV will use as many cores as possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.60      0.58      2422\n",
      "          1       0.57      0.54      0.56      2421\n",
      "\n",
      "avg / total       0.57      0.57      0.57      4843\n",
      "\n",
      "0.569275242618\n",
      "{'C': 0.0001}\n",
      "0.581664257692\n"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)\n",
    "\n",
    "print(cv.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our GridSearchCV shows a slight improvement with `C` = 0.0001 compared to the original `C`. This is the best result so far compared to decision trees and we will keep it. Experiment with other set of values and parameters, and see if you can get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Feature transformation and selection\n",
    "\n",
    "* RFECV\n",
    "* PCA\n",
    "* RFECV + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4843, 85)\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe = RFECV(estimator = LogisticRegression(C=0.0001), cv=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(rfe.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.581664257692\n",
      "0.569068759034\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.60      0.58      2422\n",
      "          1       0.57      0.54      0.55      2421\n",
      "\n",
      "avg / total       0.57      0.57      0.57      4843\n",
      "\n",
      "0.569068759034\n"
     ]
    }
   ],
   "source": [
    "X_train_sel = rfe.transform(X_train)\n",
    "X_test_sel = rfe.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(C=0.0001)\n",
    "model.fit(X_train_sel, y_train)\n",
    "y_pred = model.predict(X_test_sel)\n",
    "\n",
    "print(model.score(X_train_sel, y_train))\n",
    "print(model.score(X_test_sel, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nimport numpy as np\\n\\n\\nfrom sklearn.feature_selection import RFE\\nsel = RFE(LogisticRegression())\\nsel.fit(X_train, y_train)\\ny_pred = sel.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\nprint(accuracy_score(y_test, y_pred))\\nfrom sklearn.preprocessing import Normalizer\\nfrom sklearn.preprocessing import normalize\\ny = df['TargetB']\\nX = df.drop(['TargetB'], axis=1)\\nX_mat = normalize(X.as_matrix())\\nX_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\\nX_train\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_pred = model.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\nprint(accuracy_score(y_test, y_pred))\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_classif, chi2\\nfor i in range(2, len(names)+1):\\n    select = SelectKBest(score_func=f_classif, k=i)\\n    X_transf = select.fit_transform(X_mat, y)\\n    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\\n    model = LogisticRegression().fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    print(i, accuracy_score(y_test, y_pred))\\n    \\nnames = df.columns\\nfor i in range(2, len(names)+1):\\n    select = SelectKBest(score_func=f_classif, k=i)\\n    X_transf = select.fit_transform(X_mat, y)\\n    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\\n    model = LogisticRegression().fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    print(i, accuracy_score(y_test, y_pred))\\n    \\nfor i in range(2, len(names)):\\n    select = SelectKBest(score_func=chi2, k=i)\\n    X_transf = select.fit_transform(X_mat, y)\\n    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\\n    model = LogisticRegression().fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    print(i, accuracy_score(y_test, y_pred))\\n    \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "sel = RFE(LogisticRegression())\n",
    "sel.fit(X_train, y_train)\n",
    "y_pred = sel.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = normalize(X.as_matrix())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\n",
    "X_train\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "for i in range(2, len(names)+1):\n",
    "    select = SelectKBest(score_func=f_classif, k=i)\n",
    "    X_transf = select.fit_transform(X_mat, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(i, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "names = df.columns\n",
    "for i in range(2, len(names)+1):\n",
    "    select = SelectKBest(score_func=f_classif, k=i)\n",
    "    X_transf = select.fit_transform(X_mat, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(i, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "for i in range(2, len(names)):\n",
    "    select = SelectKBest(score_func=chi2, k=i)\n",
    "    X_transf = select.fit_transform(X_mat, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(i, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
