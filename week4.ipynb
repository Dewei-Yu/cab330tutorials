{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Regression Mining\n",
    "\n",
    "### What's on this week\n",
    "1. [Resuming from week 3](#resume)\n",
    "2. [Building your first logistic regression model](#build)\n",
    "3. [Understanding your logistic regression model](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "5. [Feature transformation and selection](#fselect)\n",
    "\n",
    "---\n",
    "\n",
    "The practical note for this week introduces you to regression mining in Python, particularly using logistic regression. Regressions are a class of linear models that learn coefficients associated with each variable/field and uses them to make predictions.\n",
    "\n",
    "**This tutorial notes is in experimental version. Please give us feedbacks and suggestions on how to make it better. Ask your tutor for any question and clarification.**\n",
    "\n",
    "## 1. Resuming from week 3 <a name=\"resume\"></a>\n",
    "Last week, we learned how to perform data mining with decision trees in Python. For this week, we will reuse the code for data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dm_tools import data_prep\n",
    "\n",
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# train test split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building your logistic regression <a name=\"build\"></a>\n",
    "There are a number of types of regression, namely linear and logistic. The type of regression to use is determined by the target's measurement level. In this case study, the target is of categorical type, thus we need to use logistic regression.\n",
    "\n",
    "Import and train your logistic regression using code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.58      0.58      2421\n",
      "          1       0.58      0.58      0.58      2422\n",
      "\n",
      "avg / total       0.58      0.58      0.58      4843\n",
      "\n",
      "0.581044806938\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty good, can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.59      0.59      2421\n",
      "          1       0.59      0.58      0.58      2422\n",
      "\n",
      "avg / total       0.59      0.59      0.59      4843\n",
      "\n",
      "0.586206896552\n",
      "{'tol': 1e-15, 'C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-4, 4)],\n",
    "         'tol': [pow(10, x) for x in range(-15, -8)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.58      0.58      2421\n",
      "          1       0.58      0.58      0.58      2422\n",
      "\n",
      "avg / total       0.58      0.58      0.58      4843\n",
      "\n",
      "0.579186454677\n",
      "{'C': 0.00021}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'C': [x*0.00001 for x in range(1, 25)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(), cv=5)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# coding: utf-8\\nimport pandas as pd\\nimport numpy as np\\n\\ndef data_prep():\\n    # read the pva97nk dataset\\n    df = pd.read_csv('pva97nk.csv')\\n\\n    # drop ID and the unused target variable\\n    df.drop(['ID', 'TargetD'], axis=1, inplace=True)\\n\\n    # impute missing values in DemAge with its mean\\n    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\\n\\n    # change DemCluster from interval/integer to nominal/str\\n    df['DemCluster'] = df['DemCluster'].astype(str)\\n\\n    # change DemHomeOwner into binary 0/1 variable\\n    dem_home_owner_map = {'U': 0, 'H': 1}\\n    df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\\n\\n    # denote miss values in DemMidIncome\\n    mask = df['DemMedIncome'] < 1\\n    df.loc[mask, 'DemMedIncome'] = np.nan\\n\\n    # df['DemMedIncome'].replace(0, np.nan, inplace=True)\\n\\n    # impute med income using average strategy\\n    df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\\n\\n    # impute gift avg card 36 using average strategy\\n    df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\\n\\n    # one hot encoding\\n    df = pd.get_dummies(df)\\n\\n    return df\\ndata_prep()\\ndf = data_prep()\\ndf['DemMedIncome']\\ny = df['TargetB']\\nX = df.drop(['TargetB'], axis=1)\\nX_mat = X.as_matrix()\\nX_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\\nfrom sklearn.model_selection import train_test_split\\ny = df['TargetB']\\nX = df.drop(['TargetB'], axis=1)\\nX_mat = X.as_matrix()\\nX_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\\nfrom sklearn.linear_model import LogisticRegression\\nmodel = LogisticRegression()\\nmodel.fit(X_train)\\nmodel.fit(X_train, y_train)\\ny_pred = model.predict(X_test)\\nfrom sklearn.metrics import classification_report, accuracy_score\\nprint(classification_report(y_test, y_pred))\\nprint(accuracy_score(y_test, y_pred))\\nfrom sklearn.feature_selection import RFE\\nsel = RFE(LogisticRegression())\\nsel.fit(X_train, y_train)\\ny_pred = sel.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\nprint(accuracy_score(y_test, y_pred))\\nfrom sklearn.preprocessing import Normalizer\\nfrom sklearn.preprocessing import normalize\\ny = df['TargetB']\\nX = df.drop(['TargetB'], axis=1)\\nX_mat = normalize(X.as_matrix())\\nX_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\\nX_train\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_pred = model.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\nprint(accuracy_score(y_test, y_pred))\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_classif, chi2\\nfor i in range(2, len(names)+1):\\n    select = SelectKBest(score_func=f_classif, k=i)\\n    X_transf = select.fit_transform(X_mat, y)\\n    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\\n    model = LogisticRegression().fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    print(i, accuracy_score(y_test, y_pred))\\n    \\nnames = df.columns\\nfor i in range(2, len(names)+1):\\n    select = SelectKBest(score_func=f_classif, k=i)\\n    X_transf = select.fit_transform(X_mat, y)\\n    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\\n    model = LogisticRegression().fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    print(i, accuracy_score(y_test, y_pred))\\n    \\nfor i in range(2, len(names)):\\n    select = SelectKBest(score_func=chi2, k=i)\\n    X_transf = select.fit_transform(X_mat, y)\\n    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\\n    model = LogisticRegression().fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    print(i, accuracy_score(y_test, y_pred))\\n    \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data_prep():\n",
    "    # read the pva97nk dataset\n",
    "    df = pd.read_csv('pva97nk.csv')\n",
    "\n",
    "    # drop ID and the unused target variable\n",
    "    df.drop(['ID', 'TargetD'], axis=1, inplace=True)\n",
    "\n",
    "    # impute missing values in DemAge with its mean\n",
    "    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
    "\n",
    "    # change DemCluster from interval/integer to nominal/str\n",
    "    df['DemCluster'] = df['DemCluster'].astype(str)\n",
    "\n",
    "    # change DemHomeOwner into binary 0/1 variable\n",
    "    dem_home_owner_map = {'U': 0, 'H': 1}\n",
    "    df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\n",
    "\n",
    "    # denote miss values in DemMidIncome\n",
    "    mask = df['DemMedIncome'] < 1\n",
    "    df.loc[mask, 'DemMedIncome'] = np.nan\n",
    "\n",
    "    # df['DemMedIncome'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # impute med income using average strategy\n",
    "    df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\n",
    "\n",
    "    # impute gift avg card 36 using average strategy\n",
    "    df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\n",
    "\n",
    "    # one hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "\n",
    "    return df\n",
    "data_prep()\n",
    "df = data_prep()\n",
    "df['DemMedIncome']\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "from sklearn.feature_selection import RFE\n",
    "sel = RFE(LogisticRegression())\n",
    "sel.fit(X_train, y_train)\n",
    "y_pred = sel.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "X_mat = normalize(X.as_matrix())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42)\n",
    "X_train\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "for i in range(2, len(names)+1):\n",
    "    select = SelectKBest(score_func=f_classif, k=i)\n",
    "    X_transf = select.fit_transform(X_mat, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(i, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "names = df.columns\n",
    "for i in range(2, len(names)+1):\n",
    "    select = SelectKBest(score_func=f_classif, k=i)\n",
    "    X_transf = select.fit_transform(X_mat, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(i, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "for i in range(2, len(names)):\n",
    "    select = SelectKBest(score_func=chi2, k=i)\n",
    "    X_transf = select.fit_transform(X_mat, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transf, y, test_size=0.5, random_state=42)\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(i, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
