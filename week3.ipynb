{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Decision Tree Mining\n",
    "\n",
    "### What's on this week\n",
    "1. [Resuming from week 2](#resume)\n",
    "2. [Building your first decision tree model](#build)\n",
    "3. [Understanding and visualizing your decision tree](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "\n",
    "---\n",
    "\n",
    "The practical note for this week introduces you to decision tree mining in Python. Decision trees are relatively simple, yet it can be powerful if build and utilised properly.\n",
    "\n",
    "**This tutorial notes is in experimental version. Please give us feedbacks and suggestions on how to make it better. Ask your tutor for any question and clarification.**\n",
    "\n",
    "## 1. Resuming from week 2 <a name=\"resume\"></a>\n",
    "Last week, we learned how to perform data preparation on our dataset. We build plots of data distribution, dealt with noisy values, imputed missing values and drop columns not required in our analysis. After week 2, your code should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside dm_tools.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_prep():\n",
    "    # read the pva97nk dataset\n",
    "    df = pd.read_csv('pva97nk.csv')\n",
    "    \n",
    "    # change DemCluster from interval/integer to nominal/str\n",
    "    df['DemCluster'] = df['DemCluster'].astype(str)\n",
    "    \n",
    "    # change DemHomeOwner into binary 0/1 variable\n",
    "    dem_home_owner_map = {'U':0, 'H': 1}\n",
    "    df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\n",
    "    \n",
    "    # denote errorneous values in DemMidIncome\n",
    "    mask = df['DemMedIncome'] < 1\n",
    "    df.loc[mask, 'DemMedIncome'] = np.nan\n",
    "    \n",
    "    # impute missing values in DemAge with its mean\n",
    "    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
    "\n",
    "    # impute med income using mean\n",
    "    df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\n",
    "\n",
    "    # impute gift avg card 36 using mean\n",
    "    df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\n",
    "    \n",
    "    # drop ID and the unused target variable\n",
    "    df.drop(['ID', 'TargetD'], axis=1, inplace=True)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in our project space\n",
    "from dm_tools import data_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building your first decision tree <a name=\"build\"></a>\n",
    "Before we build our decision tree, we need to set up our data partition first. Given a set of training data, you can easily build a model that is highly accurate in that dataset itself. However, a model built under this circumstance typically \"overfits\" the dataset, generalises poorly and has horrible accuracy in other data. To avoid this pitfall, we need to set aside some data from our whole dataset to be used as \"test\".\n",
    "\n",
    "To do this, import the `train_test_split` function from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's partition our data. The convention in Python is to split our data into `X` (variables used to learn and make prediction) and `y` (variable to be predicted). In our case, `y` would be `TargetB` and `X` would be the rest of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# train test split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you done that, convert `X` (which is still a pandas DataFrame object) into a `numpy` matrix that can be consumed by `sklearn`. Next, use the `train_test_split` function to split them into 50% training and 50% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.5, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to build our model. Let's start by importing `DecisionTreeClassifier`. We can then train the model using `.fit` function and make predictions using `.predict`. To assess performance of the model, we could use `classification_report` and `accuracy_score` of the model's prediction on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.54      0.53      2422\n",
      "          1       0.52      0.50      0.51      2421\n",
      "\n",
      "avg / total       0.52      0.52      0.52      4843\n",
      "\n",
      "0.519719182325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# simple decision tree training\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding and visualizing your decision tree\n",
    "\n",
    "### 3.1. Feature importance\n",
    "\n",
    "Let's take a deeper look on the decision tree that we just built. Firstly, we want to know what features/variables are important to the decision making process in our model. This is commonly known as \"feature importance\". In an `sklearn` decision tree, feature importance is stored within the model itself. Let's sort them in descending order and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DemMedHomeValue : 0.0960200348359\n",
      "DemMedIncome : 0.0681456588715\n",
      "GiftAvgAll : 0.0596003061126\n",
      "DemAge : 0.0594425450169\n",
      "DemPctVeterans : 0.0578280691\n",
      "GiftAvg36 : 0.0485149809775\n",
      "GiftTimeFirst : 0.0471181035703\n",
      "GiftAvgLast : 0.0390254162067\n",
      "GiftTimeLast : 0.0387218927997\n",
      "GiftCntAll : 0.0355103703122\n",
      "PromCntCard36 : 0.0350957406285\n",
      "PromCntAll : 0.034853673818\n",
      "PromCnt36 : 0.0332085974249\n",
      "GiftAvgCard36 : 0.0328464467362\n",
      "PromCntCardAll : 0.0322142581646\n",
      "GiftCnt36 : 0.0274533968633\n",
      "PromCnt12 : 0.0272177045816\n",
      "GiftCntCardAll : 0.0270905918229\n",
      "GiftCntCard36 : 0.0208595404581\n",
      "DemHomeOwner : 0.00872060013635\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In descending order, the top 3 important variables for this model are `DemMedHomeValue`, `DemPctVeterans` and `DemAge`. Feature importance is really important to not only understand the model, but also to learn more about the data and present conclusions to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Visualizing decision tree structure\n",
    "\n",
    "To understand how the decision tree looks like, we could visualize it using the `export_graphviz` and `pydot`. Open the `.png` file to view the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, it looks like the picture is too large and incomprehensible. This is because the model that we trained is very complex. Let's limit the complexity of the model by setting the `max_depth` that the model can go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.72      0.62      2422\n",
      "          1       0.59      0.40      0.48      2421\n",
      "\n",
      "avg / total       0.57      0.56      0.55      4843\n",
      "\n",
      "0.559776997729\n"
     ]
    }
   ],
   "source": [
    "#retrain with a small max_depth limit\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could see that the simpler model (small `max_depth`) actually performs much better on the test data than the complex one, showing clear example of overfitting. `max_depth` on a decision tree is what we call a hyperparameter (or just parameter) and they are responsible for the structure of a model. Different combinations of parameters will produce different models with different performance too.\n",
    "\n",
    "Let's do a feature importance analysis and visualization on this new decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftAvgLast : 0.49832551516\n",
      "DemMedHomeValue : 0.179905317927\n",
      "GiftAvgCard36 : 0.125956067503\n",
      "GiftTimeLast : 0.103066735755\n",
      "GiftCnt36 : 0.0490047340628\n",
      "DemAge : 0.0437416295918\n",
      "DemCluster_11 : 0.0\n",
      "StatusCat96NK_L : 0.0\n",
      "StatusCat96NK_N : 0.0\n",
      "StatusCat96NK_S : 0.0\n",
      "DemCluster_0 : 0.0\n",
      "DemCluster_1 : 0.0\n",
      "DemCluster_10 : 0.0\n",
      "DemCluster_13 : 0.0\n",
      "DemCluster_12 : 0.0\n",
      "StatusCat96NK_E : 0.0\n",
      "DemCluster_14 : 0.0\n",
      "DemCluster_15 : 0.0\n",
      "DemCluster_16 : 0.0\n",
      "DemCluster_17 : 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple decision tree structure](https://s3-ap-southeast-2.amazonaws.com/dataminingtuts/week3_dt_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the question is, how do we find the optimal combination of parameters for a model on a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameters with GridSearchCV\n",
    "\n",
    "A common method to find the optimal set of parameters for a model is to run a exhaustive search over all possible values of each parameter. To choose the best model among all candidates, k-fold cross validation is typically used.\n",
    "\n",
    "In k-fold cross-validation, the training dataset is randomly partitioned into `k` equal size partitions. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation, which we will use to choose the best model. (courtesy of openml.org)\n",
    "\n",
    "In `sklearn`, the exhaustive search + k-fold validation is implemented in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a GridSearchCV, we first have to determine the hyperparameters and possible values of parameters that we want to use. Each class of predictive models will have different kind of parameters (e.g. decision tree will be different to a regression). For this tutorial, we will search on 3 hyperparameters:\n",
    "1. Criterion: The function to measure the quality of a split. There are two criterias we will use, “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "2. Max depth: The maximum depth of the tree. Let's start with range of 2-10.\n",
    "3. Min samples leaf: The minimum number of samples required to be at a leaf node, allowing us to limit the minimum size of a leaf node. Let's start with range of 20-200 with step of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(3, 10),\n",
    "          'min_samples_leaf': range(20, 200, 20)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `accuracy` of our model increases slightly over the previous best. This is a promising sight, which means we might be able to further optimise on the found best parameters.\n",
    "\n",
    "Let's do another grid search, now being more specific around the best params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.62      0.59      2422\n",
      "          1       0.58      0.51      0.54      2421\n",
      "\n",
      "avg / total       0.57      0.57      0.57      4843\n",
      "\n",
      "0.567210406773\n",
      "{'max_depth': 2, 'criterion': 'gini', 'min_samples_leaf': 10}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV #2\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 5),\n",
    "          'min_samples_leaf': range(10, 30, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the `accuracy` does not improve further, which means for the 3 parameters we choose we have reached the best set. Try to experiment with other kinds of parameters and see whether you could do better than this. You can find the list of parameters available in a decision tree in `sklearn`'s `DecisionTreeClassifier` documentation website.\n",
    "\n",
    "## Tips\n",
    "\n",
    "1. Always perform feature importance and visualization to understand your decision tree. The best model can be found in `cv.best_estimator_`.\n",
    "2. We will use feature importance a lot across this decision tree modelling process. Rather than writing the script multiple times (which is tedious and error-prone), we can just wrap them in functions in `dm_tools.py` and import it from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside `dm_tools.py'\n",
    "import numpy as np\n",
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "        print(feature_names[i], ':', importances[i])\n",
    "\n",
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name) # saved in the following file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftAvgLast : 0.680580861119\n",
      "GiftAvgCard36 : 0.177381401702\n",
      "GiftTimeLast : 0.142037737179\n",
      "DemGender_U : 0.0\n",
      "DemCluster_11 : 0.0\n",
      "StatusCat96NK_F : 0.0\n",
      "StatusCat96NK_L : 0.0\n",
      "StatusCat96NK_N : 0.0\n",
      "StatusCat96NK_S : 0.0\n",
      "DemCluster_0 : 0.0\n",
      "DemCluster_1 : 0.0\n",
      "DemCluster_10 : 0.0\n",
      "DemCluster_13 : 0.0\n",
      "DemCluster_12 : 0.0\n",
      "StatusCat96NK_A : 0.0\n",
      "DemCluster_14 : 0.0\n",
      "DemCluster_15 : 0.0\n",
      "DemCluster_16 : 0.0\n",
      "DemCluster_17 : 0.0\n",
      "DemCluster_18 : 0.0\n"
     ]
    }
   ],
   "source": [
    "# do the feature importance and visualization analysis on GridSearchCV's best model\n",
    "from dm_tools import analyse_feature_importance, visualize_decision_tree\n",
    "\n",
    "analyse_feature_importance(cv.best_estimator_, X.columns, 20)\n",
    "visualize_decision_tree(cv.best_estimator_, X.columns, \"dm_best_cv.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GridSearchCV decision tree](https://s3-ap-southeast-2.amazonaws.com/dataminingtuts/dm_best_cv.png)\n",
    "\n",
    "## End notes and next week\n",
    "\n",
    "This week, we learned how to perform data partitioning, build decision trees and test them, and finding the optimal parameter sets using GridSearchCV.\n",
    "\n",
    "Next week, we will focus on performing predictive classification modelling using logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
